from __future__ import annotations

import asyncio
import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import AsyncGenerator, Literal

from app.llm.client import get_llm_client
from core.config import AppSettings, get_settings

ChunkType = Literal["answer", "summary", "detail", "done", "error"]


@dataclass
class ChatChunk:
    type: ChunkType
    text: str
    meta: dict[str, str] | None = None


class BaseOrchestrator(ABC):
    @abstractmethod
    async def stream_chat(
        self, prompt: str
    ) -> AsyncGenerator[dict[str, str | None], None]:
        """
        Stream response data for the current invocation.
        """
        ...


class FakeOrchestrator(BaseOrchestrator):
    async def stream_chat(
        self, prompt: str
    ) -> AsyncGenerator[dict[str, str | None], None]:
        chunks = [
            ChatChunk(type="answer", text=f"Thinking about {prompt}"),
            ChatChunk(type="detail", text="Gathered agent metadata"),
            ChatChunk(type="summary", text="Synthesizing a concise reply"),
            ChatChunk(type="done", text="Finished"),
        ]
        for chunk in chunks:
            await asyncio.sleep(0.2)
            yield {"type": chunk.type, "text": chunk.text}


class OpenAIOrchestrator(BaseOrchestrator):
    def __init__(self, settings: AppSettings):
        self.settings = settings
        self._llm = get_llm_client()
        logging.info(
            "OpenAI orchestrator initialized with Responses API; model=%s",
            self.settings.chat_model,
        )

    async def _stream_from_openai(self, prompt: str) -> AsyncGenerator[dict[str, str], None]:
        input_data = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ]
        
        try:
            # Responses API streaming
            async for event in self._llm.stream_response(
                input=input_data,
                model=self.settings.chat_model,
                **self._stream_kwargs(),
            ):
                # SSE event types according to Responses API
                event_type = event.get("type")

                if event_type == "response.created":
                    # Response started
                    logging.debug("OpenAI Response created: %s", event.get("response_id"))
                elif event_type == "response.output_text.delta":
                    delta = event.get("delta")
                    if delta:
                        yield {"type": "answer", "text": delta}
                elif event_type == "response.output_item.done":
                    # Item finished, could extract content if needed
                    pass
                elif event_type == "error":
                    error_msg = event.get("error", {}).get("message", "Unknown error")
                    yield {"type": "error", "text": error_msg}
                elif event_type == "response.done":
                    break
        except Exception as exc:
            logging.exception("OpenAI Responses stream failed: %s", exc)
            yield {"type": "error", "text": str(exc)}

    async def stream_chat(
        self, prompt: str
    ) -> AsyncGenerator[dict[str, str | None], None]:
        if not self.settings.openai_api_key:
            raise RuntimeError("OpenAI API key missing")

        try:
            async for chunk in self._stream_from_openai(prompt):
                yield chunk
            yield {"type": "detail", "text": "Gathered plan cues for the response"}
            yield {"type": "summary", "text": "Generated by OpenAI stream (Responses API)"}
            yield {"type": "done", "text": "OpenAI stream complete"}
        except Exception as exc:
            logging.exception("OpenAI stream failed; falling back to synchronous completion: %s", exc)
            fallback_text = await self._run_completion(prompt)
            if fallback_text:
                yield {"type": "answer", "text": fallback_text}
                yield {"type": "detail", "text": "Fallback completed plan cues"}
                yield {"type": "summary", "text": "Fallback summary; streaming not available"}
                yield {"type": "done", "text": "Fallback complete"}
                return
            logging.warning("Fallback synchronous completion failed; using fake orchestrator")
            async for chunk in FakeOrchestrator().stream_chat(prompt):
                yield chunk

    async def _run_completion(self, prompt: str) -> str | None:
        input_data = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ]
        try:
            response = await self._llm.acreate_response(
                input=input_data,
                model=self.settings.chat_model,
                **self._completion_kwargs(),
            )
            return self._llm.get_output_text(response)
        except Exception as exc:
            logging.exception("Synchronous OpenAI completion failed: %s", exc)
            return None

    def _effective_temperature(self) -> float | None:
        if "gpt-5" in self.settings.chat_model:
            return None
        return 0.1

    def _stream_kwargs(self) -> dict[str, object]:
        kwargs: dict[str, object] = {}
        temperature = self._effective_temperature()
        if temperature is not None:
            kwargs["temperature"] = temperature
        return kwargs

    def _completion_kwargs(self) -> dict[str, object]:
        kwargs: dict[str, object] = {}
        temperature = self._effective_temperature()
        if temperature is not None:
            kwargs["temperature"] = temperature
        return kwargs


def get_orchestrator() -> BaseOrchestrator:
    """
    Swap out this constructor once LangGraph or another orchestrator is wired in.
    """
    settings = get_settings()
    if settings.openai_api_key:
        return OpenAIOrchestrator(settings)
    return FakeOrchestrator()
