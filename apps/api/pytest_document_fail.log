============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-9.0.2, pluggy-1.6.0 -- /home/spa/tobit-spa-ai/apps/api/.venv/bin/python3.12
cachedir: .pytest_cache
rootdir: /home/spa/tobit-spa-ai/apps/api
configfile: pytest.ini
plugins: anyio-4.12.1, langsmith-0.6.3, asyncio-1.3.0
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

tests/test_documents.py::test_document_stream_done_contains_references FAILED [100%]

=================================== FAILURES ===================================
________________ test_document_stream_done_contains_references _________________

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
dialect = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7fa7447456d0>
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>
statement = <sqlalchemy.dialects.postgresql.psycopg.PGCompiler_psycopg object at 0x7fa730016150>
parameters = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7fa7447456d0>
cursor = <psycopg.Cursor [closed] [IDLE] (host=115.21.12.151 user=spa database=spadb) at 0x7fa73030ab10>
statement = 'INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_ty...age_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)'
parameters = {'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)

.venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:952: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <psycopg.Cursor [closed] [IDLE] (host=115.21.12.151 user=spa database=spadb) at 0x7fa73030ab10>
query = 'INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_ty...age_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)'
params = {'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}

    def execute(
        self,
        query: Query,
        params: Params | None = None,
        *,
        prepare: bool | None = None,
        binary: bool | None = None,
    ) -> Self:
        """
        Execute a query or command to the database.
        """
        try:
            with self._conn.lock:
                self._conn.wait(
                    self._execute_gen(query, params, prepare=prepare, binary=binary)
                )
        except e._NO_TRACEBACK as ex:
>           raise ex.with_traceback(None)
E           psycopg.errors.UndefinedColumn: "chunk_version" 칼럼은 "document_chunks" 릴레이션(relation)에 없음
E           LINE 1: ...unk_index, page, text, created_at, id, embedding, chunk_vers...
E                                                                        ^

.venv/lib/python3.12/site-packages/psycopg/cursor.py:117: UndefinedColumn

The above exception was the direct cause of the following exception:

setup_test_environment = <sqlmodel.orm.session.Session object at 0x7fa7305f83e0>
monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x7fa72ff9dc70>

    @pytest.mark.asyncio
    async def test_document_stream_done_contains_references(setup_test_environment, monkeypatch: pytest.MonkeyPatch):
        from api.routes import documents as documents_module
    
        class FakeSearchService:
            def __init__(self, settings):
                pass
    
            def embed_query(self, query: str) -> list[float]:
                return [0.0] * 1536
    
            def fetch_top_chunks(self, session: Session, document_id: str, top_k: int, embedding: list[float]):
                statement = select(DocumentChunk).where(DocumentChunk.document_id == document_id)
                return session.exec(statement).scalars().all()
    
            def score_chunk(self, chunk: DocumentChunk, query_embedding: list[float]) -> float:
                return 0.5
    
        class FakeOrchestrator:
            def __init__(self, settings):
                pass
    
            async def stream_chat(self, prompt: str):
                yield {"type": "answer", "text": "reference answer"}
                yield {"type": "done", "text": "complete"}
    
        monkeypatch.setattr(documents_module, "DocumentSearchService", FakeSearchService)
        monkeypatch.setattr(documents_module, "OpenAIOrchestrator", FakeOrchestrator)
    
        session = core_db.SessionLocal()
        document = Document(
            tenant_id="default",
            user_id="default",
            filename="report.pdf",
            content_type="application/pdf",
            size=123,
            status=DocumentStatus.done,
        )
        session.add(document)
        session.commit()
        session.refresh(document)
        document_id = document.id
        chunk = DocumentChunk(
            document_id=document.id,
            chunk_index=0,
            page=2,
            text="Important snippet content for reference testing.",
            embedding=[0.0] * 1536,
        )
        session.add(chunk)
>       session.commit()

tests/test_documents.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.Session object at 0x7fa72fff93d0>

    def commit(self) -> None:
        """Flush pending changes and commit the current transaction.
    
        When the COMMIT operation is complete, all objects are fully
        :term:`expired`, erasing their internal contents, which will be
        automatically re-loaded when the objects are next accessed. In the
        interim, these objects are in an expired state and will not function if
        they are :term:`detached` from the :class:`.Session`. Additionally,
        this re-load operation is not supported when using asyncio-oriented
        APIs. The :paramref:`.Session.expire_on_commit` parameter may be used
        to disable this behavior.
    
        When there is no transaction in place for the :class:`.Session`,
        indicating that no operations were invoked on this :class:`.Session`
        since the previous call to :meth:`.Session.commit`, the method will
        begin and commit an internal-only "logical" transaction, that does not
        normally affect the database unless pending flush changes were
        detected, but will still invoke event handlers and object expiration
        rules.
    
        The outermost database transaction is committed unconditionally,
        automatically releasing any SAVEPOINTs in effect.
    
        .. seealso::
    
            :ref:`session_committing`
    
            :ref:`unitofwork_transaction`
    
            :ref:`asyncio_orm_avoid_lazyloads`
    
        """
        trans = self._transaction
        if trans is None:
            trans = self._autobegin_t()
    
>       trans.commit(_to_root=True)

.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py:2030: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.SessionTransaction object at 0x7fa7301f1710>
_to_root = True

>   ???

<string>:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function SessionTransaction.commit at 0x7fa7446047c0>
self = <sqlalchemy.orm.session.SessionTransaction object at 0x7fa7301f1710>
arg = (), kw = {'_to_root': True}
current_state = <SessionTransactionState.ACTIVE: 1>
next_state = <_StateChangeStates.ANY: 1>, existing_fn = None
expect_state = <SessionTransactionState.CLOSED: 5>

    @util.decorator
    def _go(fn: _F, self: Any, *arg: Any, **kw: Any) -> Any:
        current_state = self._state
    
        if (
            has_prerequisite_states
            and current_state not in prerequisite_state_collection
        ):
            self._raise_for_prerequisite_state(fn.__name__, current_state)
    
        next_state = self._next_state
        existing_fn = self._current_fn
        expect_state = moves_to if expect_state_change else current_state
    
        if (
            # destination states are restricted
            next_state is not _StateChangeStates.ANY
            # method seeks to change state
            and expect_state_change
            # destination state incorrect
            and next_state is not expect_state
        ):
            if existing_fn and next_state in (
                _StateChangeStates.NO_CHANGE,
                _StateChangeStates.CHANGE_IN_PROGRESS,
            ):
                raise sa_exc.IllegalStateChangeError(
                    f"Method '{fn.__name__}()' can't be called here; "
                    f"method '{existing_fn.__name__}()' is already "
                    f"in progress and this would cause an unexpected "
                    f"state change to {moves_to!r}",
                    code="isce",
                )
            else:
                raise sa_exc.IllegalStateChangeError(
                    f"Cant run operation '{fn.__name__}()' here; "
                    f"will move to state {moves_to!r} where we are "
                    f"expecting {next_state!r}",
                    code="isce",
                )
    
        self._current_fn = fn
        self._next_state = _StateChangeStates.CHANGE_IN_PROGRESS
        try:
>           ret_value = fn(self, *arg, **kw)
                        ^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/sqlalchemy/orm/state_changes.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.SessionTransaction object at 0x7fa7301f1710>
_to_root = True

    @_StateChange.declare_states(
        (SessionTransactionState.ACTIVE, SessionTransactionState.PREPARED),
        SessionTransactionState.CLOSED,
    )
    def commit(self, _to_root: bool = False) -> None:
        if self._state is not SessionTransactionState.PREPARED:
            with self._expect_state(SessionTransactionState.PREPARED):
>               self._prepare_impl()

.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py:1311: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.SessionTransaction object at 0x7fa7301f1710>

>   ???

<string>:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function SessionTransaction._prepare_impl at 0x7fa7446045e0>
self = <sqlalchemy.orm.session.SessionTransaction object at 0x7fa7301f1710>
arg = (), kw = {}, current_state = <SessionTransactionState.ACTIVE: 1>
next_state = <SessionTransactionState.PREPARED: 2>
existing_fn = <function SessionTransaction.commit at 0x7fa7446047c0>
expect_state = <SessionTransactionState.PREPARED: 2>

    @util.decorator
    def _go(fn: _F, self: Any, *arg: Any, **kw: Any) -> Any:
        current_state = self._state
    
        if (
            has_prerequisite_states
            and current_state not in prerequisite_state_collection
        ):
            self._raise_for_prerequisite_state(fn.__name__, current_state)
    
        next_state = self._next_state
        existing_fn = self._current_fn
        expect_state = moves_to if expect_state_change else current_state
    
        if (
            # destination states are restricted
            next_state is not _StateChangeStates.ANY
            # method seeks to change state
            and expect_state_change
            # destination state incorrect
            and next_state is not expect_state
        ):
            if existing_fn and next_state in (
                _StateChangeStates.NO_CHANGE,
                _StateChangeStates.CHANGE_IN_PROGRESS,
            ):
                raise sa_exc.IllegalStateChangeError(
                    f"Method '{fn.__name__}()' can't be called here; "
                    f"method '{existing_fn.__name__}()' is already "
                    f"in progress and this would cause an unexpected "
                    f"state change to {moves_to!r}",
                    code="isce",
                )
            else:
                raise sa_exc.IllegalStateChangeError(
                    f"Cant run operation '{fn.__name__}()' here; "
                    f"will move to state {moves_to!r} where we are "
                    f"expecting {next_state!r}",
                    code="isce",
                )
    
        self._current_fn = fn
        self._next_state = _StateChangeStates.CHANGE_IN_PROGRESS
        try:
>           ret_value = fn(self, *arg, **kw)
                        ^^^^^^^^^^^^^^^^^^^^

.venv/lib/python3.12/site-packages/sqlalchemy/orm/state_changes.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.SessionTransaction object at 0x7fa7301f1710>

    @_StateChange.declare_states(
        (SessionTransactionState.ACTIVE,), SessionTransactionState.PREPARED
    )
    def _prepare_impl(self) -> None:
        if self._parent is None or self.nested:
            self.session.dispatch.before_commit(self.session)
    
        stx = self.session._transaction
        assert stx is not None
        if stx is not self:
            for subtransaction in stx._iterate_self_and_parents(upto=self):
                subtransaction.commit()
    
        if not self.session._flushing:
            for _flush_guard in range(100):
                if self.session._is_clean():
                    break
>               self.session.flush()

.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py:1286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.Session object at 0x7fa72fff93d0>, objects = None

    def flush(self, objects: Optional[Sequence[Any]] = None) -> None:
        """Flush all the object changes to the database.
    
        Writes out all pending object creations, deletions and modifications
        to the database as INSERTs, DELETEs, UPDATEs, etc.  Operations are
        automatically ordered by the Session's unit of work dependency
        solver.
    
        Database operations will be issued in the current transactional
        context and do not affect the state of the transaction, unless an
        error occurs, in which case the entire transaction is rolled back.
        You may flush() as often as you like within a transaction to move
        changes from Python to the database's transaction buffer.
    
        :param objects: Optional; restricts the flush operation to operate
          only on elements that are in the given collection.
    
          This feature is for an extremely narrow set of use cases where
          particular objects may need to be operated upon before the
          full flush() occurs.  It is not intended for general use.
    
        """
    
        if self._flushing:
            raise sa_exc.InvalidRequestError("Session is already flushing")
    
        if self._is_clean():
            return
        try:
            self._flushing = True
>           self._flush(objects)

.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py:4331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.Session object at 0x7fa72fff93d0>, objects = None

    def _flush(self, objects: Optional[Sequence[object]] = None) -> None:
        dirty = self._dirty_states
        if not dirty and not self._deleted and not self._new:
            self.identity_map._modified.clear()
            return
    
        flush_context = UOWTransaction(self)
    
        if self.dispatch.before_flush:
            self.dispatch.before_flush(self, flush_context, objects)
            # re-establish "dirty states" in case the listeners
            # added
            dirty = self._dirty_states
    
        deleted = set(self._deleted)
        new = set(self._new)
    
        dirty = set(dirty).difference(deleted)
    
        # create the set of all objects we want to operate upon
        if objects:
            # specific list passed in
            objset = set()
            for o in objects:
                try:
                    state = attributes.instance_state(o)
    
                except exc.NO_STATE as err:
                    raise exc.UnmappedInstanceError(o) from err
                objset.add(state)
        else:
            objset = None
    
        # store objects whose fate has been decided
        processed = set()
    
        # put all saves/updates into the flush context.  detect top-level
        # orphans and throw them into deleted.
        if objset:
            proc = new.union(dirty).intersection(objset).difference(deleted)
        else:
            proc = new.union(dirty).difference(deleted)
    
        for state in proc:
            is_orphan = _state_mapper(state)._is_orphan(state)
    
            is_persistent_orphan = is_orphan and state.has_identity
    
            if (
                is_orphan
                and not is_persistent_orphan
                and state._orphaned_outside_of_session
            ):
                self._expunge_states([state])
            else:
                _reg = flush_context.register_object(
                    state, isdelete=is_persistent_orphan
                )
                assert _reg, "Failed to add object to the flush context!"
                processed.add(state)
    
        # put all remaining deletes into the flush context.
        if objset:
            proc = deleted.intersection(objset).difference(processed)
        else:
            proc = deleted.difference(processed)
        for state in proc:
            _reg = flush_context.register_object(state, isdelete=True)
            assert _reg, "Failed to add object to the flush context!"
    
        if not flush_context.has_work:
            return
    
        flush_context.transaction = transaction = self._autobegin_t()._begin()
        try:
            self._warn_on_events = True
            try:
                flush_context.execute()
            finally:
                self._warn_on_events = False
    
            self.dispatch.after_flush(self, flush_context)
    
            flush_context.finalize_flush_changes()
    
            if not objects and self.identity_map._modified:
                len_ = len(self.identity_map._modified)
    
                statelib.InstanceState._commit_all_states(
                    [
                        (state, state.dict)
                        for state in self.identity_map._modified
                    ],
                    instance_dict=self.identity_map,
                )
                util.warn(
                    "Attribute history events accumulated on %d "
                    "previously clean instances "
                    "within inner-flush event handlers have been "
                    "reset, and will not result in database updates. "
                    "Consider using set_committed_value() within "
                    "inner-flush event handlers to avoid this warning." % len_
                )
    
            # useful assertions:
            # if not objects:
            #    assert not self.identity_map._modified
            # else:
            #    assert self.identity_map._modified == \
            #            self.identity_map._modified.difference(objects)
    
            self.dispatch.after_flush_postexec(self, flush_context)
    
            transaction.commit()
    
        except:
>           with util.safe_reraise():

.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py:4466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.util.langhelpers.safe_reraise object at 0x7fa730016110>
type_ = None, value = None, traceback = None

    def __exit__(
        self,
        type_: Optional[Type[BaseException]],
        value: Optional[BaseException],
        traceback: Optional[types.TracebackType],
    ) -> NoReturn:
        assert self._exc_info is not None
        # see #2703 for notes
        if type_ is None:
            exc_type, exc_value, exc_tb = self._exc_info
            assert exc_value is not None
            self._exc_info = None  # remove potential circular references
>           raise exc_value.with_traceback(exc_tb)

.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.session.Session object at 0x7fa72fff93d0>, objects = None

    def _flush(self, objects: Optional[Sequence[object]] = None) -> None:
        dirty = self._dirty_states
        if not dirty and not self._deleted and not self._new:
            self.identity_map._modified.clear()
            return
    
        flush_context = UOWTransaction(self)
    
        if self.dispatch.before_flush:
            self.dispatch.before_flush(self, flush_context, objects)
            # re-establish "dirty states" in case the listeners
            # added
            dirty = self._dirty_states
    
        deleted = set(self._deleted)
        new = set(self._new)
    
        dirty = set(dirty).difference(deleted)
    
        # create the set of all objects we want to operate upon
        if objects:
            # specific list passed in
            objset = set()
            for o in objects:
                try:
                    state = attributes.instance_state(o)
    
                except exc.NO_STATE as err:
                    raise exc.UnmappedInstanceError(o) from err
                objset.add(state)
        else:
            objset = None
    
        # store objects whose fate has been decided
        processed = set()
    
        # put all saves/updates into the flush context.  detect top-level
        # orphans and throw them into deleted.
        if objset:
            proc = new.union(dirty).intersection(objset).difference(deleted)
        else:
            proc = new.union(dirty).difference(deleted)
    
        for state in proc:
            is_orphan = _state_mapper(state)._is_orphan(state)
    
            is_persistent_orphan = is_orphan and state.has_identity
    
            if (
                is_orphan
                and not is_persistent_orphan
                and state._orphaned_outside_of_session
            ):
                self._expunge_states([state])
            else:
                _reg = flush_context.register_object(
                    state, isdelete=is_persistent_orphan
                )
                assert _reg, "Failed to add object to the flush context!"
                processed.add(state)
    
        # put all remaining deletes into the flush context.
        if objset:
            proc = deleted.intersection(objset).difference(processed)
        else:
            proc = deleted.difference(processed)
        for state in proc:
            _reg = flush_context.register_object(state, isdelete=True)
            assert _reg, "Failed to add object to the flush context!"
    
        if not flush_context.has_work:
            return
    
        flush_context.transaction = transaction = self._autobegin_t()._begin()
        try:
            self._warn_on_events = True
            try:
>               flush_context.execute()

.venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py:4427: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x7fa72fff9910>

    def execute(self) -> None:
        postsort_actions = self._generate_actions()
    
        postsort_actions = sorted(
            postsort_actions,
            key=lambda item: item.sort_key,
        )
        # sort = topological.sort(self.dependencies, postsort_actions)
        # print "--------------"
        # print "\ndependencies:", self.dependencies
        # print "\ncycles:", self.cycles
        # print "\nsort:", list(sort)
        # print "\nCOUNT OF POSTSORT ACTIONS", len(postsort_actions)
    
        # execute
        if self.cycles:
            for subset in topological.sort_as_subsets(
                self.dependencies, postsort_actions
            ):
                set_ = set(subset)
                while set_:
                    n = set_.pop()
                    n.execute_aggregate(self, set_)
        else:
            for rec in topological.sort(self.dependencies, postsort_actions):
>               rec.execute(self)

.venv/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py:466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = SaveUpdateAll(Mapper[DocumentChunk(document_chunks)])
uow = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x7fa72fff9910>

    @util.preload_module("sqlalchemy.orm.persistence")
    def execute(self, uow):
>       util.preloaded.orm_persistence.save_obj(
            self.mapper,
            uow.states_for_mapper_hierarchy(self.mapper, False, False),
            uow,
        )

.venv/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py:642: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

base_mapper = <Mapper at 0x7fa7347df4a0; DocumentChunk>
states = <generator object UOWTransaction.states_for_mapper_hierarchy at 0x7fa730658040>
uowtransaction = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x7fa72fff9910>
single = False

    def save_obj(base_mapper, states, uowtransaction, single=False):
        """Issue ``INSERT`` and/or ``UPDATE`` statements for a list
        of objects.
    
        This is called within the context of a UOWTransaction during a
        flush operation, given a list of states to be flushed.  The
        base mapper in an inheritance hierarchy handles the inserts/
        updates for all descendant mappers.
    
        """
    
        # if batch=false, call _save_obj separately for each object
        if not single and not base_mapper.batch:
            for state in _sort_states(base_mapper, states):
                save_obj(base_mapper, [state], uowtransaction, single=True)
            return
    
        states_to_update = []
        states_to_insert = []
    
        for (
            state,
            dict_,
            mapper,
            connection,
            has_identity,
            row_switch,
            update_version_id,
        ) in _organize_states_for_save(base_mapper, states, uowtransaction):
            if has_identity or row_switch:
                states_to_update.append(
                    (state, dict_, mapper, connection, update_version_id)
                )
            else:
                states_to_insert.append((state, dict_, mapper, connection))
    
        for table, mapper in base_mapper._sorted_tables.items():
            if table not in mapper._pks_by_table:
                continue
            insert = _collect_insert_commands(table, states_to_insert)
    
            update = _collect_update_commands(
                uowtransaction, table, states_to_update
            )
    
            _emit_update_statements(
                base_mapper,
                uowtransaction,
                mapper,
                table,
                update,
            )
    
>           _emit_insert_statements(
                base_mapper,
                uowtransaction,
                mapper,
                table,
                insert,
            )

.venv/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

base_mapper = <Mapper at 0x7fa7347df4a0; DocumentChunk>
uowtransaction = <sqlalchemy.orm.unitofwork.UOWTransaction object at 0x7fa72fff9910>
mapper = <Mapper at 0x7fa7347df4a0; DocumentChunk>
table = Table('document_chunks', MetaData(), Column('document_id', AutoString(), ForeignKey('documents.id'), table=<document_c...ash', AutoString(), table=<document_chunks>), Column('relevance_score', Float(), table=<document_chunks>), schema=None)
insert = <generator object _collect_insert_commands at 0x7fa72ffe1620>

    def _emit_insert_statements(
        base_mapper,
        uowtransaction,
        mapper,
        table,
        insert,
        *,
        bookkeeping=True,
        use_orm_insert_stmt=None,
        execution_options=None,
    ):
        """Emit INSERT statements corresponding to value lists collected
        by _collect_insert_commands()."""
    
        if use_orm_insert_stmt is not None:
            cached_stmt = use_orm_insert_stmt
            exec_opt = util.EMPTY_DICT
    
            # if a user query with RETURNING was passed, we definitely need
            # to use RETURNING.
            returning_is_required_anyway = bool(use_orm_insert_stmt._returning)
            deterministic_results_reqd = (
                returning_is_required_anyway
                and use_orm_insert_stmt._sort_by_parameter_order
            ) or bookkeeping
        else:
            returning_is_required_anyway = False
            deterministic_results_reqd = bookkeeping
            cached_stmt = base_mapper._memo(("insert", table), table.insert)
            exec_opt = {"compiled_cache": base_mapper._compiled_cache}
    
        if execution_options:
            execution_options = util.EMPTY_DICT.merge_with(
                exec_opt, execution_options
            )
        else:
            execution_options = exec_opt
    
        return_result = None
    
        for (
            (connection, _, hasvalue, has_all_pks, has_all_defaults),
            records,
        ) in groupby(
            insert,
            lambda rec: (
                rec[4],  # connection
                set(rec[2]),  # parameter keys
                bool(rec[5]),  # whether we have "value" parameters
                rec[6],
                rec[7],
            ),
        ):
            statement = cached_stmt
    
            if use_orm_insert_stmt is not None:
                statement = statement._annotate(
                    {
                        "_emit_insert_table": table,
                        "_emit_insert_mapper": mapper,
                    }
                )
    
            if (
                (
                    not bookkeeping
                    or (
                        has_all_defaults
                        or not base_mapper._prefer_eager_defaults(
                            connection.dialect, table
                        )
                        or not table.implicit_returning
                        or not connection.dialect.insert_returning
                    )
                )
                and not returning_is_required_anyway
                and has_all_pks
                and not hasvalue
            ):
                # the "we don't need newly generated values back" section.
                # here we have all the PKs, all the defaults or we don't want
                # to fetch them, or the dialect doesn't support RETURNING at all
                # so we have to post-fetch / use lastrowid anyway.
                records = list(records)
                multiparams = [rec[2] for rec in records]
    
>               result = connection.execute(
                    statement, multiparams, execution_options=execution_options
                )

.venv/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py:1048: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
statement = <sqlalchemy.sql.dml.Insert object at 0x7fa730016180>
parameters = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]

    def execute(
        self,
        statement: Executable,
        parameters: Optional[_CoreAnyExecuteParams] = None,
        *,
        execution_options: Optional[CoreExecuteOptionsParameter] = None,
    ) -> CursorResult[Any]:
        r"""Executes a SQL statement construct and returns a
        :class:`_engine.CursorResult`.
    
        :param statement: The statement to be executed.  This is always
         an object that is in both the :class:`_expression.ClauseElement` and
         :class:`_expression.Executable` hierarchies, including:
    
         * :class:`_expression.Select`
         * :class:`_expression.Insert`, :class:`_expression.Update`,
           :class:`_expression.Delete`
         * :class:`_expression.TextClause` and
           :class:`_expression.TextualSelect`
         * :class:`_schema.DDL` and objects which inherit from
           :class:`_schema.ExecutableDDLElement`
    
        :param parameters: parameters which will be bound into the statement.
         This may be either a dictionary of parameter names to values,
         or a mutable sequence (e.g. a list) of dictionaries.  When a
         list of dictionaries is passed, the underlying statement execution
         will make use of the DBAPI ``cursor.executemany()`` method.
         When a single dictionary is passed, the DBAPI ``cursor.execute()``
         method will be used.
    
        :param execution_options: optional dictionary of execution options,
         which will be associated with the statement execution.  This
         dictionary can provide a subset of the options that are accepted
         by :meth:`_engine.Connection.execution_options`.
    
        :return: a :class:`_engine.Result` object.
    
        """
        distilled_parameters = _distill_params_20(parameters)
        try:
            meth = statement._execute_on_connection
        except AttributeError as err:
            raise exc.ObjectNotExecutableError(statement) from err
        else:
>           return meth(
                self,
                distilled_parameters,
                execution_options or NO_OPTIONS,
            )

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1419: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.sql.dml.Insert object at 0x7fa730016180>
connection = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
distilled_params = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]
execution_options = {'compiled_cache': <sqlalchemy.util._collections.LRUCache object at 0x7fa72fff3dd0>}

    def _execute_on_connection(
        self,
        connection: Connection,
        distilled_params: _CoreMultiExecuteParams,
        execution_options: CoreExecuteOptionsParameter,
    ) -> Result[Any]:
        if self.supports_execution:
            if TYPE_CHECKING:
                assert isinstance(self, Executable)
>           return connection._execute_clauseelement(
                self, distilled_params, execution_options
            )

.venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:527: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
elem = <sqlalchemy.sql.dml.Insert object at 0x7fa730016180>
distilled_parameters = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]
execution_options = immutabledict({'compiled_cache': <sqlalchemy.util._collections.LRUCache object at 0x7fa72fff3dd0>})

    def _execute_clauseelement(
        self,
        elem: Executable,
        distilled_parameters: _CoreMultiExecuteParams,
        execution_options: CoreExecuteOptionsParameter,
    ) -> CursorResult[Any]:
        """Execute a sql.ClauseElement object."""
    
        execution_options = elem._execution_options.merge_with(
            self._execution_options, execution_options
        )
    
        has_events = self._has_events or self.engine._has_events
        if has_events:
            (
                elem,
                distilled_parameters,
                event_multiparams,
                event_params,
            ) = self._invoke_before_exec_event(
                elem, distilled_parameters, execution_options
            )
    
        if distilled_parameters:
            # ensure we don't retain a link to the view object for keys()
            # which links to the values, which we don't want to cache
            keys = sorted(distilled_parameters[0])
            for_executemany = len(distilled_parameters) > 1
        else:
            keys = []
            for_executemany = False
    
        dialect = self.dialect
    
        schema_translate_map = execution_options.get(
            "schema_translate_map", None
        )
    
        compiled_cache: Optional[CompiledCacheType] = execution_options.get(
            "compiled_cache", self.engine._compiled_cache
        )
    
        compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(
            dialect=dialect,
            compiled_cache=compiled_cache,
            column_keys=keys,
            for_executemany=for_executemany,
            schema_translate_map=schema_translate_map,
            linting=self.dialect.compiler_linting | compiler.WARN_LINTING,
        )
>       ret = self._execute_context(
            dialect,
            dialect.execution_ctx_cls._init_compiled,
            compiled_sql,
            distilled_parameters,
            execution_options,
            compiled_sql,
            distilled_parameters,
            elem,
            extracted_params,
            cache_hit=cache_hit,
        )

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1641: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
dialect = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7fa7447456d0>
constructor = <bound method DefaultExecutionContext._init_compiled of <class 'sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg'>>
statement = <sqlalchemy.dialects.postgresql.psycopg.PGCompiler_psycopg object at 0x7fa730016150>
parameters = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]
execution_options = immutabledict({'compiled_cache': <sqlalchemy.util._collections.LRUCache object at 0x7fa72fff3dd0>})
args = (<sqlalchemy.dialects.postgresql.psycopg.PGCompiler_psycopg object at 0x7fa730016150>, [{'chunk_index': 0, 'chunk_type...22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}], <sqlalchemy.sql.dml.Insert object at 0x7fa730016180>, [])
kw = {'cache_hit': <CacheStats.CACHE_MISS: 1>}, yp = None
conn = <sqlalchemy.pool.base._ConnectionFairy object at 0x7fa730312630>
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>

    def _execute_context(
        self,
        dialect: Dialect,
        constructor: Callable[..., ExecutionContext],
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
        execution_options: _ExecuteOptions,
        *args: Any,
        **kw: Any,
    ) -> CursorResult[Any]:
        """Create an :class:`.ExecutionContext` and execute, returning
        a :class:`_engine.CursorResult`."""
    
        if execution_options:
            yp = execution_options.get("yield_per", None)
            if yp:
                execution_options = execution_options.union(
                    {"stream_results": True, "max_row_buffer": yp}
                )
        try:
            conn = self._dbapi_connection
            if conn is None:
                conn = self._revalidate_connection()
    
            context = constructor(
                dialect, self, conn, execution_options, *args, **kw
            )
        except (exc.PendingRollbackError, exc.ResourceClosedError):
            raise
        except BaseException as e:
            self._handle_dbapi_exception(
                e, str(statement), parameters, None, None
            )
    
        if (
            self._transaction
            and not self._transaction.is_active
            or (
                self._nested_transaction
                and not self._nested_transaction.is_active
            )
        ):
            self._invalid_transaction()
    
        elif self._trans_context_manager:
            TransactionalContext._trans_ctx_check(self)
    
        if self._transaction is None:
            self._autobegin()
    
        context.pre_exec()
    
        if context.execute_style is ExecuteStyle.INSERTMANYVALUES:
            return self._exec_insertmany_context(dialect, context)
        else:
>           return self._exec_single_context(
                dialect, context, statement, parameters
            )

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
dialect = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7fa7447456d0>
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>
statement = <sqlalchemy.dialects.postgresql.psycopg.PGCompiler_psycopg object at 0x7fa730016150>
parameters = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )
    
            if self._has_events or self.engine._has_events:
                self.dispatch.after_cursor_execute(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
            context.post_exec()
    
            result = context._setup_result_proxy()
    
        except BaseException as e:
>           self._handle_dbapi_exception(
                e, str_statement, effective_parameters, cursor, context
            )

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1986: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
e = UndefinedColumn('"chunk_version" 칼럼은 "document_chunks" 릴레이션(relation)에 없음\nLINE 1: ...unk_index, page, text, created_at, id, embedding, chunk_vers...\n                                                             ^')
statement = 'INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_ty...age_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)'
parameters = {'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}
cursor = <psycopg.Cursor [closed] [IDLE] (host=115.21.12.151 user=spa database=spadb) at 0x7fa73030ab10>
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>
is_sub_exec = False

    def _handle_dbapi_exception(
        self,
        e: BaseException,
        statement: Optional[str],
        parameters: Optional[_AnyExecuteParams],
        cursor: Optional[DBAPICursor],
        context: Optional[ExecutionContext],
        is_sub_exec: bool = False,
    ) -> NoReturn:
        exc_info = sys.exc_info()
    
        is_exit_exception = util.is_exit_exception(e)
    
        if not self._is_disconnect:
            self._is_disconnect = (
                isinstance(e, self.dialect.loaded_dbapi.Error)
                and not self.closed
                and self.dialect.is_disconnect(
                    e,
                    self._dbapi_connection if not self.invalidated else None,
                    cursor,
                )
            ) or (is_exit_exception and not self.closed)
    
        invalidate_pool_on_disconnect = not is_exit_exception
    
        ismulti: bool = (
            not is_sub_exec and context.executemany
            if context is not None
            else False
        )
        if self._reentrant_error:
            raise exc.DBAPIError.instance(
                statement,
                parameters,
                e,
                self.dialect.loaded_dbapi.Error,
                hide_parameters=self.engine.hide_parameters,
                dialect=self.dialect,
                ismulti=ismulti,
            ).with_traceback(exc_info[2]) from e
        self._reentrant_error = True
        try:
            # non-DBAPI error - if we already got a context,
            # or there's no string statement, don't wrap it
            should_wrap = isinstance(e, self.dialect.loaded_dbapi.Error) or (
                statement is not None
                and context is None
                and not is_exit_exception
            )
    
            if should_wrap:
                sqlalchemy_exception = exc.DBAPIError.instance(
                    statement,
                    parameters,
                    cast(Exception, e),
                    self.dialect.loaded_dbapi.Error,
                    hide_parameters=self.engine.hide_parameters,
                    connection_invalidated=self._is_disconnect,
                    dialect=self.dialect,
                    ismulti=ismulti,
                )
            else:
                sqlalchemy_exception = None
    
            newraise = None
    
            if (self.dialect._has_events) and not self._execution_options.get(
                "skip_user_error_events", False
            ):
                ctx = ExceptionContextImpl(
                    e,
                    sqlalchemy_exception,
                    self.engine,
                    self.dialect,
                    self,
                    cursor,
                    statement,
                    parameters,
                    context,
                    self._is_disconnect,
                    invalidate_pool_on_disconnect,
                    False,
                )
    
                for fn in self.dialect.dispatch.handle_error:
                    try:
                        # handler returns an exception;
                        # call next handler in a chain
                        per_fn = fn(ctx)
                        if per_fn is not None:
                            ctx.chained_exception = newraise = per_fn
                    except Exception as _raised:
                        # handler raises an exception - stop processing
                        newraise = _raised
                        break
    
                if self._is_disconnect != ctx.is_disconnect:
                    self._is_disconnect = ctx.is_disconnect
                    if sqlalchemy_exception:
                        sqlalchemy_exception.connection_invalidated = (
                            ctx.is_disconnect
                        )
    
                # set up potentially user-defined value for
                # invalidate pool.
                invalidate_pool_on_disconnect = (
                    ctx.invalidate_pool_on_disconnect
                )
    
            if should_wrap and context:
                context.handle_dbapi_exception(e)
    
            if not self._is_disconnect:
                if cursor:
                    self._safe_close_cursor(cursor)
                # "autorollback" was mostly relevant in 1.x series.
                # It's very unlikely to reach here, as the connection
                # does autobegin so when we are here, we are usually
                # in an explicit / semi-explicit transaction.
                # however we have a test which manufactures this
                # scenario in any case using an event handler.
                # test/engine/test_execute.py-> test_actual_autorollback
                if not self.in_transaction():
                    self._rollback_impl()
    
            if newraise:
                raise newraise.with_traceback(exc_info[2]) from e
            elif should_wrap:
                assert sqlalchemy_exception is not None
>               raise sqlalchemy_exception.with_traceback(exc_info[2]) from e

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2363: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.engine.base.Connection object at 0x7fa730014f50>
dialect = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7fa7447456d0>
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>
statement = <sqlalchemy.dialects.postgresql.psycopg.PGCompiler_psycopg object at 0x7fa730016150>
parameters = [{'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -> CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
>                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <sqlalchemy.dialects.postgresql.psycopg.PGDialect_psycopg object at 0x7fa7447456d0>
cursor = <psycopg.Cursor [closed] [IDLE] (host=115.21.12.151 user=spa database=spadb) at 0x7fa73030ab10>
statement = 'INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_ty...age_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)'
parameters = {'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}
context = <sqlalchemy.dialects.postgresql.psycopg.PGExecutionContext_psycopg object at 0x7fa730015310>

    def do_execute(self, cursor, statement, parameters, context=None):
>       cursor.execute(statement, parameters)

.venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py:952: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <psycopg.Cursor [closed] [IDLE] (host=115.21.12.151 user=spa database=spadb) at 0x7fa73030ab10>
query = 'INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_ty...age_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)'
params = {'chunk_index': 0, 'chunk_type': 'text', 'chunk_version': 1, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), ...}

    def execute(
        self,
        query: Query,
        params: Params | None = None,
        *,
        prepare: bool | None = None,
        binary: bool | None = None,
    ) -> Self:
        """
        Execute a query or command to the database.
        """
        try:
            with self._conn.lock:
                self._conn.wait(
                    self._execute_gen(query, params, prepare=prepare, binary=binary)
                )
        except e._NO_TRACEBACK as ex:
>           raise ex.with_traceback(None)
E           sqlalchemy.exc.ProgrammingError: (psycopg.errors.UndefinedColumn) "chunk_version" 칼럼은 "document_chunks" 릴레이션(relation)에 없음
E           LINE 1: ...unk_index, page, text, created_at, id, embedding, chunk_vers...
E                                                                        ^
E           [SQL: INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_type, position_in_doc, page_number, slide_number, table_data, source_hash, relevance_score) VALUES (%(document_id)s::VARCHAR, %(chunk_index)s::INTEGER, %(page)s::INTEGER, %(text)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(embedding)s, %(chunk_version)s::INTEGER, %(chunk_type)s::VARCHAR, %(position_in_doc)s::INTEGER, %(page_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)]
E           [parameters: {'document_id': '248117b2-4996-43e3-b331-f74066a73b24', 'chunk_index': 0, 'page': 2, 'text': 'Important snippet content for reference testing.', 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), 'id': 'd780525f-e336-4237-bc4f-439d28335ce7', 'embedding': '[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ... (5847 characters truncated) ... ,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]', 'chunk_version': 1, 'chunk_type': 'text', 'position_in_doc': None, 'page_number': None, 'slide_number': None, 'table_data': Json(None), 'source_hash': None, 'relevance_score': None}]
E           (Background on this error at: https://sqlalche.me/e/20/f405)

.venv/lib/python3.12/site-packages/psycopg/cursor.py:117: ProgrammingError
---------------------------- Captured stderr setup -----------------------------
2026-01-22T21:43:13+0900 DEBUG asyncio request_id=- tenant_id=- trace_id=- mode=- Using selector: EpollSelector taskName=None asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 DEBUG asyncio request_id=- tenant_id=- trace_id=- mode=- Using selector: EpollSelector taskName=None asctime=2026-01-22T21:43:13+0900
------------------------------ Captured log setup ------------------------------
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
DEBUG    asyncio:selector_events.py:64 Using selector: EpollSelector
----------------------------- Captured stdout call -----------------------------
2026-01-22 21:43:13,529 INFO sqlalchemy.engine.Engine select pg_catalog.version()
2026-01-22 21:43:13,529 INFO sqlalchemy.engine.Engine [raw sql] {}
2026-01-22 21:43:13,551 INFO sqlalchemy.engine.Engine select current_schema()
2026-01-22 21:43:13,552 INFO sqlalchemy.engine.Engine [raw sql] {}
2026-01-22 21:43:13,564 INFO sqlalchemy.engine.Engine show standard_conforming_strings
2026-01-22 21:43:13,564 INFO sqlalchemy.engine.Engine [raw sql] {}
2026-01-22 21:43:13,592 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2026-01-22 21:43:13,595 INFO sqlalchemy.engine.Engine INSERT INTO documents (tenant_id, user_id, filename, content_type, size, status, error_message, created_at, updated_at, deleted_at, id, format, processing_progress, total_chunks, error_details, doc_metadata, created_by) VALUES (%(tenant_id)s::VARCHAR, %(user_id)s::VARCHAR, %(filename)s::VARCHAR, %(content_type)s::VARCHAR, %(size)s::INTEGER, %(status)s, %(error_message)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(updated_at)s::TIMESTAMP WITHOUT TIME ZONE, %(deleted_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(format)s::VARCHAR, %(processing_progress)s::INTEGER, %(total_chunks)s::INTEGER, %(error_details)s::JSON, %(doc_metadata)s::JSON, %(created_by)s::VARCHAR)
2026-01-22 21:43:13,596 INFO sqlalchemy.engine.Engine [generated in 0.00051s] {'tenant_id': 'default', 'user_id': 'default', 'filename': 'report.pdf', 'content_type': 'application/pdf', 'size': 123, 'status': 'done', 'error_message': None, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 494678, tzinfo=datetime.timezone.utc), 'updated_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 494689, tzinfo=datetime.timezone.utc), 'deleted_at': None, 'id': '248117b2-4996-43e3-b331-f74066a73b24', 'format': 'pdf', 'processing_progress': 0, 'total_chunks': 0, 'error_details': Json(None), 'doc_metadata': Json(None), 'created_by': None}
2026-01-22 21:43:13,614 INFO sqlalchemy.engine.Engine COMMIT
2026-01-22 21:43:13,620 INFO sqlalchemy.engine.Engine BEGIN (implicit)
2026-01-22 21:43:13,622 INFO sqlalchemy.engine.Engine SELECT documents.tenant_id, documents.user_id, documents.filename, documents.content_type, documents.size, documents.status, documents.error_message, documents.created_at, documents.updated_at, documents.deleted_at, documents.id, documents.format, documents.processing_progress, documents.total_chunks, documents.error_details, documents.doc_metadata, documents.created_by 
FROM documents 
WHERE documents.id = %(pk_1)s::VARCHAR
2026-01-22 21:43:13,622 INFO sqlalchemy.engine.Engine [generated in 0.00034s] {'pk_1': '248117b2-4996-43e3-b331-f74066a73b24'}
2026-01-22 21:43:13,636 INFO sqlalchemy.engine.Engine INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_type, position_in_doc, page_number, slide_number, table_data, source_hash, relevance_score) VALUES (%(document_id)s::VARCHAR, %(chunk_index)s::INTEGER, %(page)s::INTEGER, %(text)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(embedding)s, %(chunk_version)s::INTEGER, %(chunk_type)s::VARCHAR, %(position_in_doc)s::INTEGER, %(page_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)
2026-01-22 21:43:13,637 INFO sqlalchemy.engine.Engine [generated in 0.00067s] {'document_id': '248117b2-4996-43e3-b331-f74066a73b24', 'chunk_index': 0, 'page': 2, 'text': 'Important snippet content for reference testing.', 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), 'id': 'd780525f-e336-4237-bc4f-439d28335ce7', 'embedding': '[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ... (5847 characters truncated) ... ,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]', 'chunk_version': 1, 'chunk_type': 'text', 'position_in_doc': None, 'page_number': None, 'slide_number': None, 'table_data': Json(None), 'source_hash': None, 'relevance_score': None}
2026-01-22 21:43:13,642 INFO sqlalchemy.engine.Engine ROLLBACK
----------------------------- Captured stderr call -----------------------------
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- select pg_catalog.version() taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- [raw sql] {} taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- select current_schema() taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- [raw sql] {} taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- show standard_conforming_strings taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- [raw sql] {} taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- BEGIN (implicit) taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- INSERT INTO documents (tenant_id, user_id, filename, content_type, size, status, error_message, created_at, updated_at, deleted_at, id, format, processing_progress, total_chunks, error_details, doc_metadata, created_by) VALUES (%(tenant_id)s::VARCHAR, %(user_id)s::VARCHAR, %(filename)s::VARCHAR, %(content_type)s::VARCHAR, %(size)s::INTEGER, %(status)s, %(error_message)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(updated_at)s::TIMESTAMP WITHOUT TIME ZONE, %(deleted_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(format)s::VARCHAR, %(processing_progress)s::INTEGER, %(total_chunks)s::INTEGER, %(error_details)s::JSON, %(doc_metadata)s::JSON, %(created_by)s::VARCHAR) taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- [generated in 0.00051s] {'tenant_id': 'default', 'user_id': 'default', 'filename': 'report.pdf', 'content_type': 'application/pdf', 'size': 123, 'status': 'done', 'error_message': None, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 494678, tzinfo=datetime.timezone.utc), 'updated_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 494689, tzinfo=datetime.timezone.utc), 'deleted_at': None, 'id': '248117b2-4996-43e3-b331-f74066a73b24', 'format': 'pdf', 'processing_progress': 0, 'total_chunks': 0, 'error_details': Json(None), 'doc_metadata': Json(None), 'created_by': None} taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- COMMIT taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- BEGIN (implicit) taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- SELECT documents.tenant_id, documents.user_id, documents.filename, documents.content_type, documents.size, documents.status, documents.error_message, documents.created_at, documents.updated_at, documents.deleted_at, documents.id, documents.format, documents.processing_progress, documents.total_chunks, documents.error_details, documents.doc_metadata, documents.created_by 
FROM documents 
WHERE documents.id = %(pk_1)s::VARCHAR taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- [generated in 0.00034s] {'pk_1': '248117b2-4996-43e3-b331-f74066a73b24'} taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_type, position_in_doc, page_number, slide_number, table_data, source_hash, relevance_score) VALUES (%(document_id)s::VARCHAR, %(chunk_index)s::INTEGER, %(page)s::INTEGER, %(text)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(embedding)s, %(chunk_version)s::INTEGER, %(chunk_type)s::VARCHAR, %(position_in_doc)s::INTEGER, %(page_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s) taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- [generated in 0.00067s] {'document_id': '248117b2-4996-43e3-b331-f74066a73b24', 'chunk_index': 0, 'page': 2, 'text': 'Important snippet content for reference testing.', 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), 'id': 'd780525f-e336-4237-bc4f-439d28335ce7', 'embedding': '[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ... (5847 characters truncated) ... ,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]', 'chunk_version': 1, 'chunk_type': 'text', 'position_in_doc': None, 'page_number': None, 'slide_number': None, 'table_data': Json(None), 'source_hash': None, 'relevance_score': None} taskName=Task-1 asctime=2026-01-22T21:43:13+0900
2026-01-22T21:43:13+0900 INFO sqlalchemy.engine.Engine request_id=- tenant_id=- trace_id=- mode=- ROLLBACK taskName=Task-1 asctime=2026-01-22T21:43:13+0900
------------------------------ Captured log call -------------------------------
INFO     sqlalchemy.engine.Engine:base.py:1846 select pg_catalog.version()
INFO     sqlalchemy.engine.Engine:base.py:1846 [raw sql] {}
INFO     sqlalchemy.engine.Engine:base.py:1846 select current_schema()
INFO     sqlalchemy.engine.Engine:base.py:1846 [raw sql] {}
INFO     sqlalchemy.engine.Engine:base.py:1846 show standard_conforming_strings
INFO     sqlalchemy.engine.Engine:base.py:1846 [raw sql] {}
INFO     sqlalchemy.engine.Engine:base.py:2710 BEGIN (implicit)
INFO     sqlalchemy.engine.Engine:base.py:1846 INSERT INTO documents (tenant_id, user_id, filename, content_type, size, status, error_message, created_at, updated_at, deleted_at, id, format, processing_progress, total_chunks, error_details, doc_metadata, created_by) VALUES (%(tenant_id)s::VARCHAR, %(user_id)s::VARCHAR, %(filename)s::VARCHAR, %(content_type)s::VARCHAR, %(size)s::INTEGER, %(status)s, %(error_message)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(updated_at)s::TIMESTAMP WITHOUT TIME ZONE, %(deleted_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(format)s::VARCHAR, %(processing_progress)s::INTEGER, %(total_chunks)s::INTEGER, %(error_details)s::JSON, %(doc_metadata)s::JSON, %(created_by)s::VARCHAR)
INFO     sqlalchemy.engine.Engine:base.py:1846 [generated in 0.00051s] {'tenant_id': 'default', 'user_id': 'default', 'filename': 'report.pdf', 'content_type': 'application/pdf', 'size': 123, 'status': 'done', 'error_message': None, 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 494678, tzinfo=datetime.timezone.utc), 'updated_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 494689, tzinfo=datetime.timezone.utc), 'deleted_at': None, 'id': '248117b2-4996-43e3-b331-f74066a73b24', 'format': 'pdf', 'processing_progress': 0, 'total_chunks': 0, 'error_details': Json(None), 'doc_metadata': Json(None), 'created_by': None}
INFO     sqlalchemy.engine.Engine:base.py:2716 COMMIT
INFO     sqlalchemy.engine.Engine:base.py:2710 BEGIN (implicit)
INFO     sqlalchemy.engine.Engine:base.py:1846 SELECT documents.tenant_id, documents.user_id, documents.filename, documents.content_type, documents.size, documents.status, documents.error_message, documents.created_at, documents.updated_at, documents.deleted_at, documents.id, documents.format, documents.processing_progress, documents.total_chunks, documents.error_details, documents.doc_metadata, documents.created_by 
FROM documents 
WHERE documents.id = %(pk_1)s::VARCHAR
INFO     sqlalchemy.engine.Engine:base.py:1846 [generated in 0.00034s] {'pk_1': '248117b2-4996-43e3-b331-f74066a73b24'}
INFO     sqlalchemy.engine.Engine:base.py:1846 INSERT INTO document_chunks (document_id, chunk_index, page, text, created_at, id, embedding, chunk_version, chunk_type, position_in_doc, page_number, slide_number, table_data, source_hash, relevance_score) VALUES (%(document_id)s::VARCHAR, %(chunk_index)s::INTEGER, %(page)s::INTEGER, %(text)s::VARCHAR, %(created_at)s::TIMESTAMP WITHOUT TIME ZONE, %(id)s::VARCHAR, %(embedding)s, %(chunk_version)s::INTEGER, %(chunk_type)s::VARCHAR, %(position_in_doc)s::INTEGER, %(page_number)s::INTEGER, %(slide_number)s::INTEGER, %(table_data)s::JSON, %(source_hash)s::VARCHAR, %(relevance_score)s)
INFO     sqlalchemy.engine.Engine:base.py:1846 [generated in 0.00067s] {'document_id': '248117b2-4996-43e3-b331-f74066a73b24', 'chunk_index': 0, 'page': 2, 'text': 'Important snippet content for reference testing.', 'created_at': datetime.datetime(2026, 1, 22, 12, 43, 13, 634974, tzinfo=datetime.timezone.utc), 'id': 'd780525f-e336-4237-bc4f-439d28335ce7', 'embedding': '[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ... (5847 characters truncated) ... ,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]', 'chunk_version': 1, 'chunk_type': 'text', 'position_in_doc': None, 'page_number': None, 'slide_number': None, 'table_data': Json(None), 'source_hash': None, 'relevance_score': None}
INFO     sqlalchemy.engine.Engine:base.py:2713 ROLLBACK
=============================== warnings summary ===============================
core/config.py:51
core/config.py:51
  /home/spa/tobit-spa-ai/apps/api/core/config.py:51: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'env'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    document_storage_root: Optional[Path] = Field(default=None, env="DOCUMENT_STORAGE_ROOT")

.venv/lib/python3.12/site-packages/passlib/utils/__init__.py:854
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/passlib/utils/__init__.py:854: DeprecationWarning: 'crypt' is deprecated and slated for removal in Python 3.13
    from crypt import crypt as _crypt

schemas/api_manager.py:41
schemas/api_manager.py:41
  /home/spa/tobit-spa-ai/apps/api/schemas/api_manager.py:41: PydanticDeprecatedSince212: Using `@model_validator` with mode='after' on a classmethod is deprecated. Instead, use an instance method. See the documentation at https://docs.pydantic.dev/2.12/concepts/validators/#model-after-validator. Deprecated in Pydantic V2.12 to be removed in V3.0.
    @model_validator(mode="after")

.venv/lib/python3.12/site-packages/sqlmodel/main.py:534
.venv/lib/python3.12/site-packages/sqlmodel/main.py:534
.venv/lib/python3.12/site-packages/sqlmodel/main.py:534
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:534: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    new_cls = super().__new__(cls, name, bases, dict_used, **config_kwargs)

app/modules/cep_builder/schemas.py:18
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:18: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("trigger_spec", "action_spec", pre=True, always=True)

app/modules/cep_builder/schemas.py:39
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:39: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("trigger_spec", "action_spec", pre=True, always=True)

app/modules/cep_builder/schemas.py:55
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:55: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("rule_id", pre=True, always=True)

app/modules/cep_builder/schemas.py:95
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:95: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("exec_id", "rule_id", pre=True, always=True)

app/modules/cep_builder/schemas.py:112
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:112: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("headers", "trigger", "policy", pre=True, always=True)

app/modules/cep_builder/schemas.py:134
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:134: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("headers", "trigger", "policy", pre=True, always=True)

app/modules/cep_builder/schemas.py:150
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:150: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("notification_id", pre=True, always=True)

app/modules/cep_builder/schemas.py:173
  /home/spa/tobit-spa-ai/apps/api/app/modules/cep_builder/schemas.py:173: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("log_id", "notification_id", pre=True, always=True)

app/modules/api_manager/schemas.py:48
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:48: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("api_name", "method", "endpoint")

app/modules/api_manager/schemas.py:52
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:52: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("param_schema", pre=True, always=True)

app/modules/api_manager/schemas.py:56
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:56: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("runtime_policy", pre=True, always=True)

app/modules/api_manager/schemas.py:60
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:60: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("logic_spec", pre=True, always=True)

app/modules/api_manager/schemas.py:78
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:78: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("api_name", "method", "endpoint", pre=True)

app/modules/api_manager/schemas.py:82
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:82: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("param_schema", pre=True)

app/modules/api_manager/schemas.py:88
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:88: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("runtime_policy", pre=True)

app/modules/api_manager/schemas.py:94
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:94: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    @validator("logic_spec", pre=True)

app/modules/api_manager/schemas.py:101
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:101: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ApiDefinitionRead(BaseModel):

app/modules/api_manager/schemas.py:201
  /home/spa/tobit-spa-ai/apps/api/app/modules/api_manager/schemas.py:201: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/
    class ApiExecLogRead(BaseModel):

main.py:82
  /home/spa/tobit-spa-ai/apps/api/main.py:82: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("startup")

.venv/lib/python3.12/site-packages/fastapi/applications.py:4576
.venv/lib/python3.12/site-packages/fastapi/applications.py:4576
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/fastapi/applications.py:4576: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    return self.router.on_event(event_type)

main.py:119
  /home/spa/tobit-spa-ai/apps/api/main.py:119: DeprecationWarning: 
          on_event is deprecated, use lifespan event handlers instead.
  
          Read more about it in the
          [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).
          
    @app.on_event("shutdown")

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.auth.models.TbUser, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.auth.models.TbRefreshToken, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.permissions.models.TbRolePermission, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.permissions.models.TbResourcePermission, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.ci_management.models.TbCIChange, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.ci_management.models.TbCIIntegrityIssue, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.ci_management.models.TbCIDuplicate, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.audit_log.models.TbAuditLog, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.operation_settings.models.TbOperationSettings, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.inspector.models.TbExecutionTrace, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.inspector.models.TbGoldenQuery, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.inspector.models.TbRegressionBaseline, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.inspector.models.TbRegressionRun, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py::test_document_stream_done_contains_references
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/sqlmodel/main.py:632: SAWarning: This declarative base already contains a class with the same class name and module name as app.modules.inspector.models.TbRegressionRuleConfig, and will be replaced in the string-lookup table.
    DeclarativeMeta.__init__(cls, classname, bases, dict_, **kw)

tests/test_documents.py: 12 warnings
  /home/spa/tobit-spa-ai/apps/api/.venv/lib/python3.12/site-packages/pydantic/fields.py:747: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).
    return fac()

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_documents.py::test_document_stream_done_contains_references
======================== 1 failed, 56 warnings in 2.74s ========================
